{
  "metadata": {
    "name": "Formula1",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala",
      "display_name": "Spark 2 Scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala",
      "version": "2.x", 
      "display_name": "Scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n//Driver\n\n\nval path_route \u003d System.getProperty(\"user.dir\")\nval path \u003d path_route + \"/data/\"\n\n// Carga los datos CSV en un DataFrame de Spark\nvar df_drivers \u003d spark.read.option(\"sep\",\",\")\n                        .option(\"header\", \"true\")\n                        .option(\"inferSchema\", \"true\")\n                        .option(\"encoding\", \"ISO-8859-1\")\n                        .csv(path + \"drivers.csv\")\n\n// Muestra los primeros registros para verificar que se hayan leído correctamente\n//df_drivers.show()\n\ndf_drivers \u003d df_drivers.dropDuplicates(\"dateOfBirth\",\"Broadcast_Name\")\n                    .orderBy(asc(\"Broadcast_Name\"))\n                    \n\ndf_drivers \u003d df_drivers.withColumn(\"Born_Date\", to_date(col(\"dateOfBirth\")))\n                        .withColumn(\"idDriver\",monotonically_increasing_id+1)\n//z.show(df_drivers)\n\ndf_drivers \u003d df_drivers.drop(\"dateOfBirth\")\n\n// Define los parámetros de conexión a MySQL Workbench\nval usuarioMySQL \u003d \"user_vbox\"\nval contraseñaMySQL \u003d \"admin2\"\nval urlMySQL \u003d s\"jdbc:mysql://192.168.22.1:3306/ingp_formula1\"\n\n// Escribe los datos del DataFrame a la tabla de MySQL\ndf_drivers.write\n    .format(\"jdbc\")\n    .option(\"url\", urlMySQL)\n    .option(\"dbtable\", \"driver\")\n    .option(\"user\", usuarioMySQL)\n    .option(\"password\", contraseñaMySQL)\n    .mode(\"append\") // Opcional: puedes cambiar a \"overwrite\" si deseas agregar los datos a una tabla existente\n    .save()\n    \n//z.show(df_drivers)\n\n\n// Confirma que los datos se hayan cargado correctamente\nprintln(\"Los datos de driver se han cargado exitosamente en la tabla de MySQL.\")"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n\nval path_route \u003d System.getProperty(\"user.dir\")\nval path \u003d path_route + \"/data/\"\n\nvar df_format \u003d spark.read.option(\"sep\",\",\")\n                        .option(\"header\", \"true\")\n                        .option(\"inferSchema\", \"true\")\n                        .option(\"encoding\", \"UTF-8\")\n                        .csv(path + \"format.csv\")\n                        \ndf_format \u003d df_format.dropDuplicates(\"GP_Format\")\n                    .withColumn(\"idFormat\",monotonically_increasing_id+1)\n                    \ndf_format \u003d df_format.filter(col(\"GP_Format\") \u003d!\u003d \"sprint_shootout\")\n                    \ndf_format \u003d df_format.select(\"idFormat\",\"GP_Format\")\n\n//z.show(df_format)\n\n\nval usuarioMySQL \u003d \"user_vbox\"\nval contraseñaMySQL \u003d \"admin2\"\nval urlMySQL \u003d s\"jdbc:mysql://192.168.22.1:3306/ingp_formula1\"\n    \ndf_format.write\n    .format(\"jdbc\")\n    .option(\"url\", urlMySQL)\n    .option(\"dbtable\", \"format\")\n    .option(\"user\", usuarioMySQL)\n    .option(\"password\", contraseñaMySQL)\n    .mode(\"append\")\n    .save()\n    \nprintln(\"Los datos de format se han cargado exitosamente en la tabla de MySQL.\")"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n\nval path_route \u003d System.getProperty(\"user.dir\")\nval path \u003d path_route + \"/data/\"\n\nvar df_grandPrix \u003d spark.read.option(\"sep\",\",\")\n                        .option(\"header\", \"true\")\n                        .option(\"inferSchema\", \"true\")\n                        .option(\"encoding\", \"ISO-8859-1\")\n                        .csv(path + \"grand_prix.csv\")\n                        \ndf_grandPrix \u003d df_grandPrix.dropDuplicates(\"Circuit_Name\")\n                        .withColumn(\"Total_DistanceInt\",col(\"Total_Distance\").cast(\"Int\"))\n                        .withColumn(\"Country\", when(col(\"Country\") \u003d\u003d\u003d \"United States\", \"USA\").otherwise(col(\"Country\")))\n                        \ndf_grandPrix \u003d df_grandPrix.select(\"Name\",\"Circuit_Name\",\"Country\",\"Location\",\"Total_DistanceInt\",\"Total_Laps\")\n\ndf_grandPrix \u003d df_grandPrix.withColumnRenamed(\"Total_DistanceInt\",\"Total_Distance\")\n                        .withColumn(\"idGrand_Prix\", monotonically_increasing_id+1)\n                        \n//z.show(df_grandPrix)\n                    \n\nval usuarioMySQL \u003d \"user_vbox\"\nval contraseñaMySQL \u003d \"admin2\"\nval urlMySQL \u003d s\"jdbc:mysql://192.168.22.1:3306/ingp_formula1\"\n    \ndf_grandPrix.write\n    .format(\"jdbc\")\n    .option(\"url\", urlMySQL)\n    .option(\"dbtable\", \"grand_prix\")\n    .option(\"user\", usuarioMySQL)\n    .option(\"password\", contraseñaMySQL)\n    .mode(\"append\")\n    .save()\n    \nprintln(\"Los datos de Grand Prix se han cargado exitosamente en la tabla de MySQL.\")\n"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n\nval path_route \u003d System.getProperty(\"user.dir\")\nval path \u003d path_route + \"/data/\"\n\nvar df_lapDeleted \u003d spark.read.option(\"sep\",\",\")\n                        .option(\"header\", \"true\")\n                        .option(\"inferSchema\", \"true\")\n                        .option(\"encoding\", \"UTF-8\")\n                        .csv(path + \"laps.csv\")\n                        \ndf_lapDeleted \u003d df_lapDeleted.select(\"Lap_deleted\",\"Deleted_Reason\")\n\ndf_lapDeleted \u003d df_lapDeleted.dropDuplicates(\"Deleted_Reason\")\n\ndf_lapDeleted \u003d df_lapDeleted.withColumn(\"is_deleted\",when(col(\"Lap_deleted\"), \"Borrada\").otherwise(\"No borrada\"))\n                            .withColumnRenamed(\"Deleted_Reason\",\"Reason\")\n                            \n// Función UDF para extraer el valor \"TURN\" junto con el número siguiente\nval extractTurnUDF \u003d udf((description: String) \u003d\u003e {\n  if (description !\u003d null) {\n    val pattern \u003d \"\"\"TURN (\\d+)\"\"\".r\n    val result \u003d pattern.findFirstMatchIn(description)\n    result match {\n      case Some(matched) \u003d\u003e \"TURN \" + matched.group(1) // Devuelve \"TURN\" seguido del número encontrado\n      case None \u003d\u003e \"N/A\" // En caso de que no se encuentre ningún valor \"TURN\", se puede manejar devolviendo un valor por defecto, como \"N/A\"\n    }\n  } else {\n    \"N/A\" // Devuelve un valor por defecto si la descripción es nula\n  }\n})\n\n\n// Aplicar la UDF a la columna \u0027descripcion\u0027 para extraer el valor \"TURN\" junto con el número\ndf_lapDeleted \u003d df_lapDeleted.withColumn(\"New_Reason\", extractTurnUDF(col(\"Reason\")))\n\ndf_lapDeleted \u003d df_lapDeleted.dropDuplicates(\"New_Reason\")\n\ndf_lapDeleted \u003d df_lapDeleted.drop(\"Lap_deleted\",\"Reason\")\n                            .withColumn(\"idLap_deleted\", monotonically_increasing_id+1)\n\ndf_lapDeleted \u003d df_lapDeleted.withColumnRenamed(\"New_Reason\",\"Reason\")\n\n//z.show(df_lapDeleted)\n\n\nval usuarioMySQL \u003d \"user_vbox\"\nval contraseñaMySQL \u003d \"admin2\"\nval urlMySQL \u003d s\"jdbc:mysql://192.168.22.1:3306/ingp_formula1\"\n    \n\ndf_lapDeleted.write\n    .format(\"jdbc\")\n    .option(\"url\", urlMySQL)\n    .option(\"dbtable\", \"lap_deleted\")\n    .option(\"user\", usuarioMySQL)\n    .option(\"password\", contraseñaMySQL)\n    .mode(\"append\")\n    .save()\n    \nprintln(\"Los datos de las vueltas eliminadas se han cargado exitosamente en la tabla de MySQL.\")"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nimport org.apache.spark.sql.functions.row_number\nimport org.apache.spark.sql.expressions.Window\n\nval path_route \u003d System.getProperty(\"user.dir\")\nval path \u003d path_route + \"/data/\"\n\nvar df_lapNumber \u003d spark.read.option(\"sep\",\",\")\n                        .option(\"header\", \"true\")\n                        .option(\"inferSchema\", \"true\")\n                        .option(\"encoding\", \"UTF-8\")\n                        .csv(path + \"grand_prix.csv\")\n                        \ndf_lapNumber \u003d df_lapNumber.select(\"Total_Laps\")\n\nval mayor \u003d df_lapNumber.agg(max(\"Total_Laps\")).collect()(0)(0).asInstanceOf[Int]\n\ndf_lapNumber \u003d spark.range(1, mayor+1).toDF(\"Number\")\n\nval window \u003d Window.orderBy(\"Number\")\ndf_lapNumber \u003d df_lapNumber.withColumn(\"idLap_number\", row_number().over(window))\n\n\n//df_lapNumber \u003d df_lapNumber.withColumn(\"idLap_number\", monotonically_increasing_id+1)\n//z.show(df_lapNumber)\n\nval usuarioMySQL \u003d \"user_vbox\"\nval contraseñaMySQL \u003d \"admin2\"\nval urlMySQL \u003d s\"jdbc:mysql://192.168.22.1:3306/ingp_formula1\"\n    \ndf_lapNumber.write\n    .format(\"jdbc\")\n    .option(\"url\", urlMySQL)\n    .option(\"dbtable\", \"lap_number\")\n    .option(\"user\", usuarioMySQL)\n    .option(\"password\", contraseñaMySQL)\n    .mode(\"append\")\n    .save()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n\nval valores \u003d Seq(\"Parada\",\"No parada\")\n\nvar df_pitStop \u003d spark.createDataFrame(valores.map(Tuple1.apply)).toDF(\"Lap_Stop\")\n\ndf_pitStop \u003d df_pitStop.withColumn(\"idPit_stop\", monotonically_increasing_id+1)\n                            \nval usuarioMySQL \u003d \"user_vbox\"\nval contraseñaMySQL \u003d \"admin2\"\nval urlMySQL \u003d s\"jdbc:mysql://192.168.22.1:3306/ingp_formula1\"\n                            \n\ndf_pitStop.write\n    .format(\"jdbc\")\n    .option(\"url\", urlMySQL)\n    .option(\"dbtable\", \"pit_stop\")\n    .option(\"user\", usuarioMySQL)\n    .option(\"password\", contraseñaMySQL)\n    .mode(\"append\") // Opcional: puedes cambiar a \"overwrite\" si deseas agregar los datos a una tabla existente\n    .save()\n    \nprintln(\"Los datos de la tabla pit stop se han cargado exitosamente en la tabla de MySQL.\")\n"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n\nimport java.time.LocalDate\nimport org.apache.spark.sql.functions._\nimport spark.implicits._\n\n// Definir los años para los cuales se generarán las fechas\nval years \u003d Seq(2022, 2023)\n\n// DataFrame con todas las fechas de los años especificados, teniendo en cuenta años bisiestos\nval year_dates \u003d years.flatMap { year \u003d\u003e\n  val startDate \u003d LocalDate.of(year, 1, 1)\n  val endDate \u003d LocalDate.of(year, 12, 31)\n  \n  // Secuencia de fechas para el año actual, considerando si es bisiesto o no\n  val datesForYear \u003d (0L to java.time.temporal.ChronoUnit.DAYS.between(startDate, endDate)).map { day \u003d\u003e\n    startDate.plusDays(day)\n  }\n  \n  // Pasar la secuencia de fechas a un DataFrame\n  datesForYear.map(date \u003d\u003e (date.getDayOfMonth, date.getMonthValue, date.getYear))\n}.toDF(\"Day\", \"Month\", \"Year\")\n\n/* Mostrar el DataFrame con las columnas dia, mes y año\nyear_dates.show(year_dates.count.toInt, false)\n*/\n\nval df_schedule \u003d year_dates.withColumn(\"idSchedule\",monotonically_increasing_id+1)\n\n                            \nval usuarioMySQL \u003d \"user_vbox\"\nval contraseñaMySQL \u003d \"admin2\"\nval urlMySQL \u003d s\"jdbc:mysql://192.168.22.1:3306/ingp_formula1\"\n                            \n\ndf_schedule.write\n    .format(\"jdbc\")\n    .option(\"url\", urlMySQL)\n    .option(\"dbtable\", \"schedule\")\n    .option(\"user\", usuarioMySQL)\n    .option(\"password\", contraseñaMySQL)\n    .mode(\"append\") // Opcional: puedes cambiar a \"overwrite\" si deseas agregar los datos a una tabla existente\n    .save()\n    \nprintln(\"Los datos de schedule se han cargado exitosamente en la tabla de MySQL.\")"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n\nval path_route \u003d System.getProperty(\"user.dir\")\nval path \u003d path_route + \"/data/\"\n\nvar df_team \u003d spark.read.option(\"sep\",\",\")\n                        .option(\"header\", \"true\")\n                        .option(\"inferSchema\", \"true\")\n                        .option(\"encoding\", \"UTF-8\")\n                        .csv(path + \"team.csv\")\n                        \ndf_team \u003d df_team.dropDuplicates(\"TeamId\")\n\ndf_team \u003d df_team.select(\"Name\",\"Nationality\")\n\ndf_team \u003d df_team.withColumn(\"idTeam\",monotonically_increasing_id+1)\n\nval usuarioMySQL \u003d \"user_vbox\"\nval contraseñaMySQL \u003d \"admin2\"\nval urlMySQL \u003d s\"jdbc:mysql://192.168.22.1:3306/ingp_formula1\"\n    \ndf_team.write\n    .format(\"jdbc\")\n    .option(\"url\", urlMySQL)\n    .option(\"dbtable\", \"team\")\n    .option(\"user\", usuarioMySQL)\n    .option(\"password\", contraseñaMySQL)\n    .mode(\"append\")\n    .save()\n    \n"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n\nval path_route \u003d System.getProperty(\"user.dir\")\nval path \u003d path_route + \"/data/\"\n\nvar df_trackStatus \u003d spark.read.option(\"sep\",\",\")\n                        .option(\"header\", \"true\")\n                        .option(\"inferSchema\", \"true\")\n                        .option(\"encoding\", \"UTF-8\")\n                        .csv(path + \"track_status.csv\")\n                        \ndf_trackStatus\u003d df_trackStatus.dropDuplicates(\"Status\")\n\nval usuarioMySQL \u003d \"user_vbox\"\nval contraseñaMySQL \u003d \"admin2\"\nval urlMySQL \u003d s\"jdbc:mysql://192.168.22.1:3306/ingp_formula1\"\n    \ndf_trackStatus.write\n    .format(\"jdbc\")\n    .option(\"url\", urlMySQL)\n    .option(\"dbtable\", \"track_status\")\n    .option(\"user\", usuarioMySQL)\n    .option(\"password\", contraseñaMySQL)\n    .mode(\"append\")\n    .save()\n    \nprintln(\"Los datos del estado de la pista se han cargado exitosamente en la tabla de MySQL.\")"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n\nval path_route \u003d System.getProperty(\"user.dir\")\nval path \u003d path_route + \"/data/\"\n\nvar df_tyre \u003d spark.read.option(\"sep\",\",\")\n                        .option(\"header\", \"true\")\n                        .option(\"inferSchema\", \"true\")\n                        .option(\"encoding\", \"UTF-8\")\n                        .csv(path + \"laps.csv\")\n                        \ndf_tyre \u003d df_tyre.select(\"Tyre\")\n\ndf_tyre \u003d df_tyre.dropDuplicates(\"Tyre\")\n\ndf_tyre \u003d df_tyre.withColumnRenamed(\"Tyre\",\"Compound\")\n                .withColumn(\"idTyre\", monotonically_increasing_id+1)\n\nval usuarioMySQL \u003d \"user_vbox\"\nval contraseñaMySQL \u003d \"admin2\"\nval urlMySQL \u003d s\"jdbc:mysql://192.168.22.1:3306/ingp_formula1\"\n    \ndf_tyre.write\n    .format(\"jdbc\")\n    .option(\"url\", urlMySQL)\n    .option(\"dbtable\", \"tyre\")\n    .option(\"user\", usuarioMySQL)\n    .option(\"password\", contraseñaMySQL)\n    .mode(\"append\")\n    .save()\n    \nprintln(\"Los datos de los tipos de neumáticos se han cargado exitosamente en la tabla de MySQL.\")"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n\nval valores \u003d Seq(\"Soleado\",\"Lluvia\")\n\nvar df_weather \u003d spark.createDataFrame(valores.map(Tuple1.apply)).toDF(\"Weather_Condition\")\n\ndf_weather \u003d df_weather.withColumn(\"idWeather\", monotonically_increasing_id+1)\n                            \nval usuarioMySQL \u003d \"user_vbox\"\nval contraseñaMySQL \u003d \"admin2\"\nval urlMySQL \u003d s\"jdbc:mysql://192.168.22.1:3306/ingp_formula1\"\n                            \n\ndf_weather.write\n    .format(\"jdbc\")\n    .option(\"url\", urlMySQL)\n    .option(\"dbtable\", \"weather\")\n    .option(\"user\", usuarioMySQL)\n    .option(\"password\", contraseñaMySQL)\n    .mode(\"append\") // Opcional: puedes cambiar a \"overwrite\" si deseas agregar los datos a una tabla existente\n    .save()\n    \nprintln(\"Los datos de la tabla de weather se han cargado exitosamente en la tabla de MySQL.\")\n"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nimport scala.math.BigDecimal.RoundingMode\nimport org.apache.spark.sql.expressions.Window\nimport org.apache.spark.sql.DataFrame\nimport org.apache.spark.sql.{SparkSession, DataFrame}\n\nval path_route \u003d System.getProperty(\"user.dir\")\nval path \u003d path_route + \"/data/\"\n\nvar df_tablaHechos \u003d spark.read.option(\"sep\",\",\")\n                        .option(\"header\", \"true\")\n                        .option(\"inferSchema\", \"true\")\n                        .option(\"encoding\", \"ISO-8859-1\")\n                        .csv(path + \"laps.csv\")\n                        \nval usuarioMySQL \u003d \"user_vbox\"\nval contraseñaMySQL \u003d \"admin2\"\nval urlMySQL \u003d s\"jdbc:mysql://192.168.22.1:3306/ingp_formula1\"\n\nval extractTurnUDF \u003d udf((description: String) \u003d\u003e {\n  if (description !\u003d null) {\n    val pattern \u003d \"\"\"TURN (\\d+)\"\"\".r\n    val result \u003d pattern.findFirstMatchIn(description)\n    result match {\n      case Some(matched) \u003d\u003e \"TURN \" + matched.group(1) \n      case None \u003d\u003e \"N/A\" \n    }\n  } else {\n    \"N/A\" \n  }\n})\n\ndf_tablaHechos \u003d df_tablaHechos.withColumn(\"DateSearch\", \n                                when(hour(col(\"Date\")) \u003d\u003d\u003d 6 \u0026\u0026 minute(col(\"Date\")) \u003d\u003d\u003d 0,\n                                    to_date(date_sub(to_timestamp(col(\"Date\"), \"yyyy-MM-dd HH:mm\"), 1))\n                                ).otherwise(to_date(col(\"Date\")))\n                            )\n                            .withColumn(\"WeatherSearch\", when(col(\"Weather\"), \"Lluvia\").otherwise(\"Soleado\"))\n                            .withColumn(\"Lap_deletedSearch\", when(col(\"Lap_deleted\"), \"Borrada\").otherwise(\"No borrada\"))\n                            .withColumn(\"Deleted_ReasonSearch\", extractTurnUDF(col(\"Deleted_Reason\")))\n                            \ndf_tablaHechos \u003d df_tablaHechos.drop(\"Date\",\"Weather\",\"Deleted_Reason\",\"Lap_deleted\",\"Time_OnTrack\")\n                        \nval df_driver \u003d spark.read\n  .format(\"jdbc\")\n  .option(\"url\", urlMySQL)\n  .option(\"dbtable\", \"driver\") \n  .option(\"user\", usuarioMySQL)\n  .option(\"password\", contraseñaMySQL)\n  .load()\n\ndf_tablaHechos \u003d df_tablaHechos.join(df_driver,df_tablaHechos(\"Driver\") \u003d\u003d\u003d df_driver(\"Broadcast_Name\"), \"inner\")\n                            .select(df_tablaHechos(\"*\"), df_driver(\"idDriver\").alias(\"idDriver\"))\n                            \nval df_team \u003d spark.read\n  .format(\"jdbc\")\n  .option(\"url\", urlMySQL)\n  .option(\"dbtable\", \"team\") \n  .option(\"user\", usuarioMySQL)\n  .option(\"password\", contraseñaMySQL)\n  .load()\n  \ndf_tablaHechos \u003d df_tablaHechos.join(df_team,df_tablaHechos(\"Team\") \u003d\u003d\u003d df_team(\"Name\"), \"inner\")\n                            .select(df_tablaHechos(\"*\"), df_team(\"idTeam\").alias(\"idTeam\"))\n                            \nval df_formatCSV \u003d spark.read.option(\"sep\",\",\")\n                        .option(\"header\", \"true\")\n                        .option(\"inferSchema\", \"true\")\n                        .option(\"encoding\", \"UTF-8\")\n                        .csv(path + \"format.csv\")\n\nval df_formatDB \u003d spark.read\n  .format(\"jdbc\")\n  .option(\"url\", urlMySQL)\n  .option(\"dbtable\", \"format\") \n  .option(\"user\", usuarioMySQL)\n  .option(\"password\", contraseñaMySQL)\n  .load()\n  \nval df_formatCSV2 \u003d df_formatCSV.withColumn(\"GP_name\",when(col(\"GP_Format\") \u003d\u003d\u003d \"sprint_shootout\" , \"sprint\").otherwise(col(\"GP_Format\")))\n                                .drop(\"GP_Format\")\n  \nvar df_format \u003d df_formatCSV2.join(df_formatDB,df_formatCSV2(\"GP_name\") \u003d\u003d\u003d df_formatDB(\"GP_Format\"), \"inner\")\n                            .select(df_formatCSV2(\"*\"), df_formatDB(\"idFormat\").alias(\"idFormat\"))\n                            \ndf_format \u003d df_format.withColumn(\"Date\", to_date(col(\"Fecha_Evento\")))\n\ndf_tablaHechos \u003d df_tablaHechos.join(df_format,df_tablaHechos(\"DateSearch\") \u003d\u003d\u003d df_format(\"Date\"), \"inner\")\n                            .select(df_tablaHechos(\"*\"), df_format(\"idFormat\").alias(\"idFormat\"))\n\nval df_grandP\u003d spark.read\n  .format(\"jdbc\")\n  .option(\"url\", urlMySQL)\n  .option(\"dbtable\", \"grand_prix\") \n  .option(\"user\", usuarioMySQL)\n  .option(\"password\", contraseñaMySQL)\n  .load()\n  \n/*\ndf_format \u003d df_grandP.orderBy(\"Name\")\ndf_tablaHechos \u003d df_tablaHechos.orderBy(\"Grand_Prix\")\n\nval uniqueValues \u003d df_format.select(\"Name\").distinct().collect().map(_.getString(0)).toList\nprintln(uniqueValues)\n\nprintln(\"separador\")\n\nval uniqueValues2 \u003d df_tablaHechos.select(\"Grand_Prix\").distinct().collect().map(_.getString(0)).toList\nprintln(uniqueValues2)\n*/\n  \ndf_tablaHechos \u003d df_tablaHechos.join(df_grandP,df_tablaHechos(\"Grand_Prix\") \u003d\u003d\u003d df_grandP(\"Name\"), \"inner\")\n                            .select(df_tablaHechos(\"*\"), df_grandP(\"idGrand_Prix\").alias(\"idGrand_Prix\"))\n                \nval df_tyre \u003d spark.read\n  .format(\"jdbc\")\n  .option(\"url\", urlMySQL)\n  .option(\"dbtable\", \"tyre\") \n  .option(\"user\", usuarioMySQL)\n  .option(\"password\", contraseñaMySQL)\n  .load()\n  \ndf_tablaHechos \u003d df_tablaHechos.join(df_tyre,df_tablaHechos(\"Tyre\") \u003d\u003d\u003d df_tyre(\"Compound\"), \"inner\")\n                            .select(df_tablaHechos(\"*\"), df_tyre(\"idTyre\").alias(\"idTyre\"))\n                         \nvar df_schedule \u003d spark.read\n  .format(\"jdbc\")\n  .option(\"url\", urlMySQL)\n  .option(\"dbtable\", \"schedule\") \n  .option(\"user\", usuarioMySQL)\n  .option(\"password\", contraseñaMySQL)\n  .load()\n  \ndf_schedule \u003d df_schedule.withColumn(\"Date\", \n                    to_date(concat($\"Year\", lit(\"-\"), $\"Month\", lit(\"-\"), $\"Day\")))\n                    \ndf_tablaHechos \u003d df_tablaHechos.join(df_schedule, df_tablaHechos(\"DateSearch\") \u003d\u003d\u003d df_schedule(\"Date\"), \"inner\")\n                            .select(df_tablaHechos(\"*\"), df_schedule(\"idSchedule\").alias(\"idSchedule\"))\n\ndf_tablaHechos \u003d df_tablaHechos.drop(\"Driver\",\"Team\",\"Grand_Prix\",\"Tyre\",\"DateSearch\")\n                            \nval df_weather \u003d spark.read\n  .format(\"jdbc\")\n  .option(\"url\", urlMySQL)\n  .option(\"dbtable\", \"weather\") \n  .option(\"user\", usuarioMySQL)\n  .option(\"password\", contraseñaMySQL)\n  .load()\n  \ndf_tablaHechos \u003d df_tablaHechos.join(df_weather,df_tablaHechos(\"WeatherSearch\") \u003d\u003d\u003d df_weather(\"Weather_Condition\"), \"inner\")\n\nval df_lapDelet \u003d spark.read\n  .format(\"jdbc\")\n  .option(\"url\", urlMySQL)\n  .option(\"dbtable\", \"lap_deleted\") \n  .option(\"user\", usuarioMySQL)\n  .option(\"password\", contraseñaMySQL)\n  .load()\n \ndf_tablaHechos \u003d df_tablaHechos.join(df_lapDelet,df_tablaHechos(\"Lap_deletedSearch\") \u003d\u003d\u003d df_lapDelet(\"is_deleted\") \u0026\u0026 df_tablaHechos(\"Deleted_ReasonSearch\") \u003d\u003d\u003d df_lapDelet(\"Reason\"), \"inner\")\n                            .select(df_tablaHechos(\"*\"), df_lapDelet(\"idLap_deleted\").alias(\"idLap_deleted\"))\n\n// Definir una función para aplicar la lógica de prioridad\ndef prioridad(col1: Int, col2: Int): Int \u003d {\n  if (col2 \u003d\u003d 1 \u0026\u0026 col1 \u003d\u003d 12) {\n    1\n  } else {\n    val prioridades \u003d Array(5, 4, 6, 7, 2, 1)\n    val col1Array \u003d col1.toString.split(\"\").map(_.toInt)\n    prioridades.find(col1Array.contains).getOrElse(0)\n  }\n}\n\n// Definir la función de prioridad como una UDF (User Defined Function)\nval prioridadUDF \u003d udf((col1: Int, col2: Int) \u003d\u003e prioridad(col1, col2))\n\n// Aplicar la UDF para generar la nueva columna con prioridad\ndf_tablaHechos \u003d df_tablaHechos.withColumn(\"idTrack_statusOld\", prioridadUDF(col(\"Track_status\"), col(\"Lap_Number\")))\n\nval df_trackStatus \u003d spark.read\n  .format(\"jdbc\")\n  .option(\"url\", urlMySQL)\n  .option(\"dbtable\", \"track_status\") \n  .option(\"user\", usuarioMySQL)\n  .option(\"password\", contraseñaMySQL)\n  .load()\n\ndf_tablaHechos \u003d df_tablaHechos.join(df_trackStatus,df_tablaHechos(\"idTrack_statusOld\") \u003d\u003d\u003d df_trackStatus(\"idTrack_status\"), \"inner\")\n                            .select(df_tablaHechos(\"*\"), df_trackStatus(\"idTrack_status\").alias(\"idTrack_status\"))\n\n// Definir una función UDF para convertir la cadena de tiempo a segundos\nval toSecondsUDF \u003d udf((timeString: String) \u003d\u003e {\n  val timeRegex \u003d \"\"\"(\\d+) days (\\d+):(\\d+):(\\d+.\\d+)\"\"\".r\n  timeString match {\n    case timeRegex(days, hours, minutes, seconds) \u003d\u003e\n      val totalSeconds \u003d days.toInt * 24 * 60 * 60 +\n        hours.toInt * 60 * 60 +\n        minutes.toInt * 60 +\n        seconds.toDouble\n      BigDecimal(totalSeconds).setScale(3, RoundingMode.HALF_UP).toDouble\n    case _ \u003d\u003e 0.0 // Manejar el caso en el que la cadena no coincide con el formato esperado\n  }\n})\n\n\ndf_tablaHechos\u003d df_tablaHechos.withColumn(\"Lap_time\", toSecondsUDF(col(\"Lap_time\")))\n                            .withColumn(\"Sector1\", toSecondsUDF(col(\"Sector_1\")))\n                            .withColumn(\"Sector2\", toSecondsUDF(col(\"Sector_2\")))\n                            .withColumn(\"Sector3\", toSecondsUDF(col(\"Sector_3\")))\n                            .withColumnRenamed(\"Lap_Number\",\"idLap_Number\")\n                            .withColumnRenamed(\"Avg_Speed\",\"Avg_SpeedOld\")\n                            .withColumnRenamed(\"Avg_RPM\",\"Avg_RPMOld\")\n                            .withColumnRenamed(\"Avg_Gear\",\"Avg_GearOld\")\n                            .withColumn(\"Pit_stop\", when(col(\"Pit_Out\") \u003d!\u003d \"NaT\", \"Parada\").otherwise(\"No parada\"))\n                            .drop(\"idTrack_statusOld\")\n            \ndf_tablaHechos \u003d df_tablaHechos.drop(\"WeatherSearch\",\"Weather_Condition\",\"Lap_deletedSearch\",\"Deleted_ReasonSearch\",\"Sector_1\",\"Sector_2\",\"Sector_3\",\"Track_status\")\n\nval df_pitstop \u003d spark.read\n  .format(\"jdbc\")\n  .option(\"url\", urlMySQL)\n  .option(\"dbtable\", \"pit_stop\") \n  .option(\"user\", usuarioMySQL)\n  .option(\"password\", contraseñaMySQL)\n  .load()\n\ndf_tablaHechos \u003d df_tablaHechos.join(df_pitstop,df_tablaHechos(\"Pit_stop\") \u003d\u003d\u003d df_pitstop(\"Lap_stop\"), \"inner\")\n                            .select(df_tablaHechos(\"*\"), df_pitstop(\"idPit_stop\").alias(\"idPit_stop\"))\n\ndf_tablaHechos \u003d df_tablaHechos.withColumnRenamed(\"Speed_S1\",\"SpeedTrap_S1\")\n                            .withColumnRenamed(\"Speed_S2\",\"SpeedTrap_S2\")\n                            .withColumnRenamed(\"Track_Temperature\",\"Track_Temp\")\n                            .withColumnRenamed(\"Speed_FL\",\"SpeedTrap_FL\")\n                            .withColumnRenamed(\"Speed_LS\",\"SpeedTrap_LS\")\n                            .withColumn(\"Avg_Speed\", when(col(\"Avg_SpeedOld\").isNull.or(col(\"Avg_SpeedOld\") \u003d\u003d\u003d \"Sin datos\"), col(\"Avg_SpeedOld\"))\n                                            .otherwise(round(col(\"Avg_SpeedOld\"), 2)))\n                            .withColumn(\"Avg_RPM\", when(col(\"Avg_RPMOld\").isNull.or(col(\"Avg_RPMOld\") \u003d\u003d\u003d \"Sin datos\"), col(\"Avg_RPMOld\"))\n                                            .otherwise(round(col(\"Avg_RPMOld\"), 2)))                \n                            .withColumn(\"Avg_Gear\", when(col(\"Avg_GearOld\").isNull.or(col(\"Avg_GearOld\") \u003d\u003d\u003d \"Sin datos\"), col(\"Avg_GearOld\"))\n                                            .otherwise(round(col(\"Avg_GearOld\"), 2)))\n                            .withColumn(\"Brake_lap\", when(col(\"Brake_percent\").isNull.or(col(\"Brake_percent\") \u003d\u003d\u003d \"Sin datos\"), col(\"Brake_percent\"))\n                                            .otherwise(round(col(\"Brake_percent\"), 2)))\n                            .withColumn(\"DRS_on\", when(col(\"DRS_On_Percent\").isNull.or(col(\"DRS_On_Percent\") \u003d\u003d\u003d \"Sin datos\"), col(\"DRS_On_Percent\"))\n                                            .otherwise(round(col(\"DRS_On_Percent\"), 2)))\n                            .withColumn(\"InSearch\", toSecondsUDF(col(\"Pit_In\")))\n                            .withColumn(\"OutSearch\", toSecondsUDF(col(\"Pit_Out\")))\n\ndf_tablaHechos \u003d df_tablaHechos.drop(\"Pit_stop\",\"Avg_SpeedOld\",\"Avg_RPMOld\",\"Avg_GearOld\",\"Brake_percent\",\"DRS_On_Percent\",\"Pit_In\",\"Pit_Out\")\n                            .withColumn(\"idtabla_hechos\", row_number().over(Window.orderBy(\"idSchedule\",\"idDriver\",\"idLap_Number\")))\n                            .withColumn(\"nLaps\",lit(1))\n\ndef calculatePitTime(df: DataFrame): DataFrame \u003d {\n  val windowSpec \u003d Window.orderBy(\"idtabla_hechos\")\n  val resultDF \u003d df.withColumn(\"Pit_time\", when(col(\"OutSearch\") \u003d!\u003d 0, round(col(\"OutSearch\") - lag(\"InSearch\", 1, 0).over(windowSpec) ,3)).otherwise(0))\n  resultDF\n}\n\nval df_useful \u003d df_tablaHechos.select(\"idtabla_hechos\",\"OutSearch\",\"InSearch\")\n\nval final_df \u003d calculatePitTime(df_useful)\n\ndf_tablaHechos \u003d df_tablaHechos.join(final_df,df_tablaHechos(\"idtabla_hechos\") \u003d\u003d\u003d final_df(\"idtabla_hechos\"), \"inner\")\n                            .select(df_tablaHechos(\"*\"), final_df(\"Pit_time\").alias(\"Pit_time\"))\n                            \nval sel_Column \u003d df_tablaHechos.select(\"Lap_time\",\"Sector1\",\"Sector2\",\"Sector3\",\"idtabla_hechos\")\n                                      \ndf_tablaHechos \u003d df_tablaHechos.drop(\"Lap_time\",\"Sector1\",\"Sector2\",\"Sector3\",\"InSearch\",\"OutSearch\")\n                            .orderBy(\"idtabla_hechos\")\n\n// Definir una nueva columna que indique si solo una de las columnas tiene un valor cero\nval df_with_condition \u003d sel_Column.withColumn(\"One_zero\", \n    (when(col(\"Lap_time\") \u003d\u003d\u003d 0 \u0026\u0026 col(\"Sector1\") \u003d!\u003d 0 \u0026\u0026 col(\"Sector2\") \u003d!\u003d 0 \u0026\u0026 col(\"Sector3\") \u003d!\u003d 0, lit(1))\n    .when(col(\"Sector1\") \u003d\u003d\u003d 0 \u0026\u0026 col(\"Lap_time\") \u003d!\u003d 0 \u0026\u0026 col(\"Sector2\") \u003d!\u003d 0 \u0026\u0026 col(\"Sector3\") \u003d!\u003d 0, lit(1))\n    .when(col(\"Sector2\") \u003d\u003d\u003d 0 \u0026\u0026 col(\"Lap_time\") \u003d!\u003d 0 \u0026\u0026 col(\"Sector1\") \u003d!\u003d 0 \u0026\u0026 col(\"Sector3\") \u003d!\u003d 0, lit(1))\n    .when(col(\"Sector3\") \u003d\u003d\u003d 0 \u0026\u0026 col(\"Lap_time\") \u003d!\u003d 0 \u0026\u0026 col(\"Sector1\") \u003d!\u003d 0 \u0026\u0026 col(\"Sector2\") \u003d!\u003d 0, lit(1))\n    .otherwise(lit(0)))\n)\n\n// Calcular el valor faltante en la columna correspondiente y sobrescribir el valor cero\nval df_with_calculated_value \u003d df_with_condition.withColumn(\"Lap_time\", \n    when(col(\"One_zero\") \u003d\u003d\u003d 1 \u0026\u0026 col(\"Lap_time\") \u003d\u003d\u003d 0, round(col(\"Sector1\") + col(\"Sector2\") + col(\"Sector3\"), 3))\n    .otherwise(col(\"Lap_time\"))\n).withColumn(\"Sector1\", \n    when(col(\"One_zero\") \u003d\u003d\u003d 1 \u0026\u0026 col(\"Sector1\") \u003d\u003d\u003d 0, round(col(\"Lap_time\") - col(\"Sector2\") - col(\"Sector3\"), 3))\n    .otherwise(col(\"Sector1\"))\n).withColumn(\"Sector2\", \n    when(col(\"One_zero\") \u003d\u003d\u003d 1 \u0026\u0026 col(\"Sector2\") \u003d\u003d\u003d 0, round(col(\"Lap_time\") - col(\"Sector1\") - col(\"Sector3\"), 3))\n    .otherwise(col(\"Sector2\"))\n).withColumn(\"Sector3\", \n    when(col(\"One_zero\") \u003d\u003d\u003d 1 \u0026\u0026 col(\"Sector3\") \u003d\u003d\u003d 0, round(col(\"Lap_time\") - col(\"Sector1\") - col(\"Sector2\"), 3))\n    .otherwise(col(\"Sector3\"))\n)\n\ndf_tablaHechos \u003d df_tablaHechos.join(df_with_calculated_value,df_tablaHechos(\"idtabla_hechos\") \u003d\u003d\u003d df_with_calculated_value(\"idtabla_hechos\"), \"inner\")\n                            .select(df_tablaHechos(\"*\"), \n                                    df_with_calculated_value(\"Lap_time\").alias(\"Lap_time\"), \n                                    df_with_calculated_value(\"Sector1\").alias(\"Sector1\"), \n                                    df_with_calculated_value(\"Sector2\").alias(\"Sector2\"), \n                                    df_with_calculated_value(\"Sector3\").alias(\"Sector3\"))\n\ndf_tablaHechos \u003d df_tablaHechos.drop(\"idtabla_hechos\")\n\ndf_tablaHechos \u003d df_tablaHechos.withColumn(\"id\", row_number().over(Window.orderBy(\"idSchedule\",\"idTeam\",\"idLap_Number\")))\n                            \ndef calculateDifferences(df: DataFrame): DataFrame \u003d {\n  val windowSpec \u003d Window.orderBy(\"id\")\n  val resultDF \u003d df.withColumn(\"LapDifference_teammate\", \n                                round(when(col(\"idLap_Number\") \u003d\u003d\u003d lead(\"idLap_Number\", 1).over(windowSpec), \n                                     col(\"Lap_time\") - lead(\"Lap_time\", 1, 0).over(windowSpec))\n                                .otherwise(col(\"Lap_time\") - lag(\"Lap_time\", 1, 0).over(windowSpec)), 3)\n                            )\n                   .withColumn(\"Sec1Difference_teammate\", \n                                round(when(col(\"idLap_Number\") \u003d\u003d\u003d lead(\"idLap_Number\", 1).over(windowSpec), \n                                     col(\"Sector1\") - lead(\"Sector1\", 1, 0).over(windowSpec))\n                                .otherwise(col(\"Sector1\") - lag(\"Sector1\", 1, 0).over(windowSpec)), 3)\n                            )\n                   .withColumn(\"Sec2Difference_teammate\", \n                                round(when(col(\"idLap_Number\") \u003d\u003d\u003d lead(\"idLap_Number\", 1).over(windowSpec), \n                                     col(\"Sector2\") - lead(\"Sector2\", 1, 0).over(windowSpec))\n                                .otherwise(col(\"Sector2\") - lag(\"Sector2\", 1, 0).over(windowSpec)), 3)\n                            )\n                   .withColumn(\"Sec3Difference_teammate\", \n                                round(when(col(\"idLap_Number\") \u003d\u003d\u003d lead(\"idLap_Number\", 1).over(windowSpec), \n                                     col(\"Sector3\") - lead(\"Sector3\", 1, 0).over(windowSpec))\n                                .otherwise(col(\"Sector3\") - lag(\"Sector3\", 1, 0).over(windowSpec)), 3)\n                            )\n  resultDF\n}\n\n//when(col(\"id\") % 2 \u003d\u003d\u003d 1 || \n\nval result_df1 \u003d calculateDifferences(df_tablaHechos)\n\ndf_tablaHechos \u003d df_tablaHechos.join(result_df1, df_tablaHechos(\"id\") \u003d\u003d\u003d result_df1(\"id\"), \"inner\")\n                              .select(df_tablaHechos(\"*\"),\n                                      result_df1(\"LapDifference_teammate\").alias(\"LapDifference_teammate\"), \n                                      result_df1(\"Sec1Difference_teammate\").alias(\"Sec1Difference_teammate\"), \n                                      result_df1(\"Sec2Difference_teammate\").alias(\"Sec2Difference_teammate\"), \n                                      result_df1(\"Sec3Difference_teammate\").alias(\"Sec3Difference_teammate\"))\n\ndf_tablaHechos \u003d df_tablaHechos.drop(\"id\")\n\ndf_tablaHechos \u003d df_tablaHechos.withColumn(\"idtabla_hechos\", row_number().over(Window.orderBy(\"idSchedule\",\"idDriver\",\"idLap_Number\")))\n                            \nval columnsToReplace2 \u003d Seq(\"Avg_Speed\",\"Avg_Gear\",\"Avg_RPM\",\"Brake_lap\",\"DRS_on\")\n                            \ndf_tablaHechos \u003d columnsToReplace2.foldLeft(df_tablaHechos) { (df, colName) \u003d\u003e\n  df.withColumn(colName, when(col(colName) \u003d\u003d\u003d \"Sin datos\", lit(null)).otherwise(col(colName).cast(\"decimal(10,2)\")))\n}\n\nval result_df \u003d df_tablaHechos.select(\"SpeedTrap_LS\",\"SpeedTrap_FL\",\"SpeedTrap_S1\",\"SpeedTrap_S2\",\"idtabla_hechos\",\"idDriver\",\"Avg_Speed\",\"Avg_Gear\",\"Avg_RPM\",\"Brake_lap\",\"DRS_on\")\n\ndf_tablaHechos \u003d df_tablaHechos.drop(\"SpeedTrap_LS\",\"SpeedTrap_FL\",\"SpeedTrap_S1\",\"SpeedTrap_S2\",\"Avg_Speed\",\"Avg_Gear\",\"Avg_RPM\",\"Brake_lap\",\"DRS_on\")\n\ndef calculateSpeedTrapMean(df_mean: DataFrame): DataFrame \u003d {\n    // Definir una ventana para ordenar por \"idtabla_hechos\"\n    val windowSpec \u003d Window.orderBy(\"idtabla_hechos\")\n\n    // Calcular las medias de los valores anteriores y posteriores para cada columna\n    val df_with_mean_speed \u003d df_mean.withColumn(\"prev_SpeedTrap_LS\", lag(\"SpeedTrap_LS\", 1).over(windowSpec))\n                                    .withColumn(\"next_SpeedTrap_LS\", lead(\"SpeedTrap_LS\", 1).over(windowSpec))\n                                    .withColumn(\"prev_SpeedTrap_S1\", lag(\"SpeedTrap_S1\", 1).over(windowSpec))\n                                    .withColumn(\"next_SpeedTrap_S1\", lead(\"SpeedTrap_S1\", 1).over(windowSpec))\n                                    .withColumn(\"prev_SpeedTrap_FL\", lag(\"SpeedTrap_FL\", 1).over(windowSpec))\n                                    .withColumn(\"next_SpeedTrap_FL\", lead(\"SpeedTrap_FL\", 1).over(windowSpec))\n                                    .withColumn(\"prev_SpeedTrap_S2\", lag(\"SpeedTrap_S2\", 1).over(windowSpec))\n                                    .withColumn(\"next_SpeedTrap_S2\", lead(\"SpeedTrap_S2\", 1).over(windowSpec))\n                                    .withColumn(\"mean_SpeedTrap_LS\", \n                                        (col(\"prev_SpeedTrap_LS\") + col(\"next_SpeedTrap_LS\")) / 2\n                                    )\n                                    .withColumn(\"mean_SpeedTrap_S1\", \n                                        (col(\"prev_SpeedTrap_S1\") + col(\"next_SpeedTrap_S1\")) / 2\n                                    )\n                                    .withColumn(\"mean_SpeedTrap_FL\", \n                                        (col(\"prev_SpeedTrap_FL\") + col(\"next_SpeedTrap_FL\")) / 2\n                                    )\n                                    .withColumn(\"mean_SpeedTrap_S2\", \n                                        (col(\"prev_SpeedTrap_S2\") + col(\"next_SpeedTrap_S2\")) / 2\n                                    )\n                                    .withColumn(\"prev_Avg_Speed\", lag(\"Avg_Speed\", 1).over(windowSpec))\n                                    .withColumn(\"next_Avg_Speed\", lead(\"Avg_Speed\", 1).over(windowSpec))\n                                    .withColumn(\"prev_Avg_Gear\", lag(\"Avg_Gear\", 1).over(windowSpec))\n                                    .withColumn(\"next_Avg_Gear\", lead(\"Avg_Gear\", 1).over(windowSpec))\n                                    .withColumn(\"prev_Avg_RPM\", lag(\"Avg_RPM\", 1).over(windowSpec))\n                                    .withColumn(\"next_Avg_RPM\", lead(\"Avg_RPM\", 1).over(windowSpec))\n                                    .withColumn(\"prev_Brake_lap\", lag(\"Brake_lap\", 1).over(windowSpec))\n                                    .withColumn(\"next_Brake_lap\", lead(\"Brake_lap\", 1).over(windowSpec))\n                                    .withColumn(\"prev_DRS_on\", lag(\"DRS_on\", 1).over(windowSpec))\n                                    .withColumn(\"next_DRS_on\", lead(\"DRS_on\", 1).over(windowSpec))\n                                    .withColumn(\"mean_Avg_Speed\", \n                                        (col(\"prev_Avg_Speed\") + col(\"next_Avg_Speed\")) / 2\n                                    )\n                                    .withColumn(\"mean_Avg_Gear\", \n                                        (col(\"prev_Avg_Gear\") + col(\"next_Avg_Gear\")) / 2\n                                    )\n                                    .withColumn(\"mean_Avg_RPM\", \n                                        (col(\"prev_Avg_RPM\") + col(\"next_Avg_RPM\")) / 2\n                                    )\n                                    .withColumn(\"mean_Brake_lap\", \n                                        (col(\"prev_Brake_lap\") + col(\"next_Brake_lap\")) / 2\n                                    )\n                                    .withColumn(\"mean_DRS_on\", \n                                        (col(\"prev_DRS_on\") + col(\"next_DRS_on\")) / 2\n                                    )\n    \n      // Reemplazar los valores nulos en cada columna según las condiciones establecidas\n    val df_final \u003d df_with_mean_speed.withColumn(\"SpeedTrap_LS\", \n        when(col(\"SpeedTrap_LS\").isNaN,\n            when(col(\"mean_SpeedTrap_LS\").isNotNull \u0026\u0026 col(\"mean_SpeedTrap_LS\").isNaN \u003d\u003d\u003d false,\n                col(\"mean_SpeedTrap_LS\")\n            ).otherwise(\n                when(\n                    col(\"prev_SpeedTrap_LS\").isNotNull \u0026\u0026 col(\"idDriver\") \u003d\u003d\u003d lag(\"idDriver\", 1).over(windowSpec) \u0026\u0026 col(\"prev_SpeedTrap_LS\").isNaN \u003d\u003d\u003d false,\n                    col(\"prev_SpeedTrap_LS\")\n                ).otherwise(\n                    when(\n                        col(\"next_SpeedTrap_LS\").isNotNull \u0026\u0026 col(\"idDriver\") \u003d\u003d\u003d lead(\"idDriver\", 1).over(windowSpec) \u0026\u0026 col(\"next_SpeedTrap_LS\").isNaN \u003d\u003d\u003d false,\n                        col(\"next_SpeedTrap_LS\")\n                    ).otherwise(lit(\"Mal\"))\n                )\n            )\n        ).otherwise(col(\"SpeedTrap_LS\"))\n    ).withColumn(\"SpeedTrap_S1\", \n        when(col(\"SpeedTrap_S1\").isNaN,\n            when(col(\"mean_SpeedTrap_S1\").isNotNull \u0026\u0026 col(\"mean_SpeedTrap_S1\").isNaN \u003d\u003d\u003d false,\n                col(\"mean_SpeedTrap_S1\")\n            ).otherwise(\n                when(\n                    col(\"prev_SpeedTrap_S1\").isNotNull \u0026\u0026 col(\"idDriver\") \u003d\u003d\u003d lag(\"idDriver\", 1).over(windowSpec) \u0026\u0026 col(\"prev_SpeedTrap_S1\").isNaN \u003d\u003d\u003d false,\n                    col(\"prev_SpeedTrap_S1\")\n                ).otherwise(\n                    when(\n                        col(\"next_SpeedTrap_S1\").isNotNull \u0026\u0026 col(\"idDriver\") \u003d\u003d\u003d lead(\"idDriver\", 1).over(windowSpec) \u0026\u0026 col(\"next_SpeedTrap_S1\").isNaN \u003d\u003d\u003d false,\n                        col(\"next_SpeedTrap_S1\")\n                    ).otherwise(lit(\"Mal\"))\n                )\n            )\n        ).otherwise(col(\"SpeedTrap_S1\"))\n    ).withColumn(\"SpeedTrap_FL\", \n        when(col(\"SpeedTrap_FL\").isNaN,\n            when(col(\"mean_SpeedTrap_FL\").isNotNull \u0026\u0026 col(\"mean_SpeedTrap_FL\").isNaN \u003d\u003d\u003d false,\n                col(\"mean_SpeedTrap_FL\")\n            ).otherwise(\n                when(\n                    col(\"prev_SpeedTrap_FL\").isNotNull \u0026\u0026 col(\"idDriver\") \u003d\u003d\u003d lag(\"idDriver\", 1).over(windowSpec) \u0026\u0026 col(\"prev_SpeedTrap_FL\").isNaN \u003d\u003d\u003d false,\n                    col(\"prev_SpeedTrap_FL\")\n                ).otherwise(\n                    when(\n                        col(\"next_SpeedTrap_FL\").isNotNull \u0026\u0026 col(\"idDriver\") \u003d\u003d\u003d lead(\"idDriver\", 1).over(windowSpec) \u0026\u0026 col(\"next_SpeedTrap_FL\").isNaN \u003d\u003d\u003d false,\n                        col(\"next_SpeedTrap_FL\")\n                    ).otherwise(lit(\"Mal\"))\n                )\n            )\n        ).otherwise(col(\"SpeedTrap_FL\"))\n    ).withColumn(\"SpeedTrap_S2\", \n        when(col(\"SpeedTrap_S2\").isNaN,\n            when(col(\"mean_SpeedTrap_S2\").isNotNull \u0026\u0026 col(\"mean_SpeedTrap_S2\").isNaN \u003d\u003d\u003d false,\n                col(\"mean_SpeedTrap_S2\")\n            ).otherwise(\n                when(\n                    col(\"prev_SpeedTrap_S2\").isNotNull \u0026\u0026 col(\"idDriver\") \u003d\u003d\u003d lag(\"idDriver\", 1).over(windowSpec) \u0026\u0026 col(\"prev_SpeedTrap_S2\").isNaN \u003d\u003d\u003d false,\n                    col(\"prev_SpeedTrap_S2\")\n                ).otherwise(\n                    when(\n                        col(\"next_SpeedTrap_S2\").isNotNull \u0026\u0026 col(\"idDriver\") \u003d\u003d\u003d lead(\"idDriver\", 1).over(windowSpec) \u0026\u0026 col(\"next_SpeedTrap_S2\").isNaN \u003d\u003d\u003d false,\n                        col(\"next_SpeedTrap_S2\")\n                    ).otherwise(lit(\"Mal\"))\n                )\n            )\n        ).otherwise(col(\"SpeedTrap_S2\"))\n    ).withColumn(\"Avg_Speed\", \n        when(col(\"Avg_Speed\").isNull,\n            when(col(\"mean_Avg_Speed\").isNotNull,\n                col(\"mean_Avg_Speed\")\n            ).otherwise(\n                when(\n                    col(\"prev_Avg_Speed\").isNotNull \u0026\u0026 col(\"idDriver\") \u003d\u003d\u003d lag(\"idDriver\", 1).over(windowSpec),\n                    col(\"prev_Avg_Speed\")\n                ).otherwise(\n                    when(\n                        col(\"next_Avg_Speed\").isNotNull \u0026\u0026 col(\"idDriver\") \u003d\u003d\u003d lead(\"idDriver\", 1).over(windowSpec),\n                        col(\"next_Avg_Speed\")\n                    ).otherwise(lit(\"Mal\"))\n                )\n            )\n        ).otherwise(col(\"Avg_Speed\"))\n    ).withColumn(\"Avg_Gear\", \n        when(col(\"Avg_Gear\").isNull,\n            when(col(\"mean_Avg_Gear\").isNotNull,\n                col(\"mean_Avg_Gear\")\n            ).otherwise(\n                when(\n                    col(\"prev_Avg_Gear\").isNotNull \u0026\u0026 col(\"idDriver\") \u003d\u003d\u003d lag(\"idDriver\", 1).over(windowSpec),\n                    col(\"prev_Avg_Gear\")\n                ).otherwise(\n                    when(\n                        col(\"next_Avg_Gear\").isNotNull \u0026\u0026 col(\"idDriver\") \u003d\u003d\u003d lead(\"idDriver\", 1).over(windowSpec),\n                        col(\"next_Avg_Gear\")\n                    ).otherwise(lit(\"Mal\"))\n                )\n            )\n        ).otherwise(col(\"Avg_Gear\"))\n    ).withColumn(\"Avg_RPM\", \n        when(col(\"Avg_RPM\").isNull,\n            when(col(\"mean_Avg_RPM\").isNotNull,\n                col(\"mean_Avg_RPM\")\n            ).otherwise(\n                when(\n                    col(\"prev_Avg_RPM\").isNotNull \u0026\u0026 col(\"idDriver\") \u003d\u003d\u003d lag(\"idDriver\", 1).over(windowSpec),\n                    col(\"prev_Avg_RPM\")\n                ).otherwise(\n                    when(\n                        col(\"next_Avg_RPM\").isNotNull \u0026\u0026 col(\"idDriver\") \u003d\u003d\u003d lead(\"idDriver\", 1).over(windowSpec),\n                        col(\"next_Avg_RPM\")\n                    ).otherwise(lit(\"Mal\"))\n                )\n            )\n        ).otherwise(col(\"Avg_RPM\"))\n    ).withColumn(\"Brake_lap\", \n        when(col(\"Brake_lap\").isNull,\n            when(col(\"mean_Brake_lap\").isNotNull,\n                col(\"mean_Brake_lap\")\n            ).otherwise(\n                when(\n                    col(\"prev_Brake_lap\").isNotNull \u0026\u0026 col(\"idDriver\") \u003d\u003d\u003d lag(\"idDriver\", 1).over(windowSpec),\n                    col(\"prev_Brake_lap\")\n                ).otherwise(\n                    when(\n                        col(\"next_Brake_lap\").isNotNull \u0026\u0026 col(\"idDriver\") \u003d\u003d\u003d lead(\"idDriver\", 1).over(windowSpec),\n                        col(\"next_Brake_lap\")\n                    ).otherwise(lit(\"Mal\"))\n                )\n            )\n        ).otherwise(col(\"Brake_lap\"))\n    ).withColumn(\"DRS_on\", \n        when(col(\"DRS_on\").isNull,\n            when(col(\"mean_DRS_on\").isNotNull,\n                col(\"mean_DRS_on\")\n            ).otherwise(\n                when(\n                    col(\"prev_DRS_on\").isNotNull \u0026\u0026 col(\"idDriver\") \u003d\u003d\u003d lag(\"idDriver\", 1).over(windowSpec),\n                    col(\"prev_DRS_on\")\n                ).otherwise(\n                    when(\n                        col(\"next_DRS_on\").isNotNull \u0026\u0026 col(\"idDriver\") \u003d\u003d\u003d lead(\"idDriver\", 1).over(windowSpec),\n                        col(\"next_DRS_on\")\n                    ).otherwise(lit(\"Mal\"))\n                )\n            )\n        ).otherwise(col(\"DRS_on\"))\n    )\n\n    df_final\n}\n\nval result_2 \u003d calculateSpeedTrapMean(result_df)\n\n\ndf_tablaHechos \u003d df_tablaHechos.join(result_2, df_tablaHechos(\"idtabla_hechos\") \u003d\u003d\u003d result_2(\"idtabla_hechos\"), \"inner\")\n                              .select(df_tablaHechos(\"*\"),\n                                      result_2(\"SpeedTrap_LS\").alias(\"SpeedTrap_LS\"), \n                                      result_2(\"SpeedTrap_S1\").alias(\"SpeedTrap_S1\"), \n                                      result_2(\"SpeedTrap_FL\").alias(\"SpeedTrap_FL\"), \n                                      result_2(\"SpeedTrap_S2\").alias(\"SpeedTrap_S2\"),\n                                      result_2(\"Avg_Speed\").alias(\"Avg_Speed\"), \n                                      result_2(\"Avg_Gear\").alias(\"Avg_Gear\"), \n                                      result_2(\"Avg_RPM\").alias(\"Avg_RPM\"), \n                                      result_2(\"Brake_lap\").alias(\"Brake_lap\"), \n                                      result_2(\"DRS_on\").alias(\"DRS_on\"))\n                                      \ndef eliminarFilasMalas(df: DataFrame): DataFrame \u003d {\n    // Aplicar filtro para eliminar las filas que contienen 0 en alguna de las columnas\n    val dfBueno \u003d df.filter(\n        $\"Lap_time\" \u003d!\u003d 0 \u0026\u0026 $\"Sector1\" \u003d!\u003d 0 \u0026\u0026 $\"Sector2\" \u003d!\u003d 0 \u0026\u0026 $\"Sector3\" \u003d!\u003d 0\n    )\n    \n    // Contar las filas eliminadas\n    val filasEliminadas \u003d df.count() - dfBueno.count()\n    println(s\"Se eliminaron $filasEliminadas filas.\")\n    \n    // Devolver el DataFrame con las filas válidas\n    dfBueno\n}\n\ndf_tablaHechos \u003d eliminarFilasMalas(df_tablaHechos)\n\ndef eliminarFilasMalas2(df: DataFrame): DataFrame \u003d {\n    // Aplicar filtro para eliminar las filas que contienen \"Mal\" en alguna de las columnas\n    val dfBueno \u003d df.filter(\n        $\"SpeedTrap_S1\" \u003d!\u003d \"Mal\" \u0026\u0026 $\"SpeedTrap_S2\" \u003d!\u003d \"Mal\" \u0026\u0026 $\"SpeedTrap_FL\" \u003d!\u003d \"Mal\" \u0026\u0026 $\"SpeedTrap_LS\" \u003d!\u003d \"Mal\"\n    )\n    \n    val filasEliminadas \u003d df.count() - dfBueno.count()\n    println(s\"Se eliminaron $filasEliminadas filas.\")\n    \n    dfBueno\n}\n\ndf_tablaHechos \u003d eliminarFilasMalas2(df_tablaHechos)\n\n.withColumn(\"id\", row_number().over(Window.orderBy(\"idSchedule\",\"idTeam\",\"idLap_Number\")))\n\ndf_tablaHechos \u003d df_tablaHechos.withColumnRenamed(\"idtabla_hechos\",\"idtabla_hechosOld\")\n\ndf_tablaHechos \u003d df_tablaHechos.withColumn(\"id\", row_number().over(Window.orderBy(\"idtabla_hechosOld\")))\n\ndf_tablaHechos \u003d df_tablaHechos.withColumn(\"idTrack_status\", when(col(\"idTrack_status\").isNull, 1).otherwise(col(\"idTrack_status\")))\n                                .drop(\"idtabla_hechosOld\")\n\n//z.show(df_tablaHechos)\n\ndf_tablaHechos \u003d df_tablaHechos.orderBy(\"idtabla_hechos\")\nprintln(df_tablaHechos.count())\n//println(df_tablaHechos.take(df_tablaHechos.count().toInt).last)\n\nval df \u003d df_tablaHechos.write\n    .format(\"jdbc\")\n    .option(\"url\", urlMySQL)\n    .option(\"dbtable\", \"tabla_hechos\")\n    .option(\"user\", usuarioMySQL)\n    .option(\"password\", contraseñaMySQL)\n    .mode(\"append\") // Opcional: puedes cambiar a \"overwrite\" si deseas agregar los datos a una tabla existente\n    .save()\n    \nprintln(\"Los datos de la tabla de hechos se han cargado exitosamente en la tabla de MySQL.\")\n\n                        \n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n"
    }
  ]
}